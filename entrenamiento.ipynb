{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish') \n",
    "palabras_ignoradas = [\"?\",\"¿\",\"!\",\"¡\"]\n",
    "archivo_datos = open(\"intents.json\").read()\n",
    "intenciones = json.loads(archivo_datos)\n",
    "\n",
    "def tokenizador():\n",
    "    palabras = []\n",
    "    categorias = []\n",
    "    documentos = []\n",
    "\n",
    "    for intencion in intenciones[\"intenciones\"]:\n",
    "        for patron in intencion[\"patterns\"]:\n",
    "            w = nltk.word_tokenize(patron)\n",
    "            palabras.extend(w)\n",
    "            documentos.append((w, intencion[\"categoria\"])) \n",
    "            if intencion[\"categoria\"] not in categorias: \n",
    "                categorias.append(intencion[\"categoria\"]) \n",
    "\n",
    "    return palabras, categorias, documentos\n",
    "\n",
    "def lematizador(palabras, categorias, documentos):\n",
    "    palabras = [stemmer.stem(w.lower()) for w in palabras if w not in palabras_ignoradas]\n",
    "    palabras2 = palabras\n",
    "    pickle.dump(palabras, open(\"palabras.pkl\", \"wb\"))\n",
    "    pickle.dump(categorias, open(\"categorias.pkl\", \"wb\"))\n",
    "    return palabras2\n",
    "\n",
    "def entrenamiento(palabras, categorias, documentos):\n",
    "    entrenamiento = []\n",
    "    salida_vacia = [0] * len(categorias)\n",
    "    for doc in documentos:\n",
    "        bolsa = []\n",
    "        palabras_patron = doc[0]\n",
    "        palabras_patron = [stemmer.stem(palabra.lower()) for palabra in palabras_patron if palabra not in palabras_ignoradas]\n",
    "        for w in palabras:\n",
    "            bolsa.append(1) if w in palabras_patron else bolsa.append(0)\n",
    "        salida_fila = list(salida_vacia)\n",
    "        salida_fila[categorias.index(doc[1])] = 1\n",
    "        entrenamiento.append([bolsa, salida_fila])\n",
    "    entrenamiento = np.array(entrenamiento, dtype=object)\n",
    "    x_entrenamiento = list(entrenamiento[:,0])\n",
    "    y_entrenamiento = list(entrenamiento[:,1])\n",
    "    return x_entrenamiento, y_entrenamiento\n",
    "\n",
    "def construir_modelo(x_entrenamiento, y_entrenamiento, categorias):\n",
    "    modelo = Sequential()\n",
    "    modelo.add(Dense(128, input_shape=(len(x_entrenamiento[0]),), activation='relu'))\n",
    "    modelo.add(Dropout(0.5))\n",
    "    modelo.add(Dense(64, activation='relu'))\n",
    "    modelo.add(Dropout(0.5))\n",
    "    modelo.add(Dense(len(categorias), activation='softmax'))\n",
    "    modelo.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    historial = modelo.fit(np.array(x_entrenamiento), np.array(y_entrenamiento), epochs=50, batch_size=5, verbose=1, validation_split=0.16)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.plot(historial.history['accuracy'])\n",
    "    plt.plot(historial.history['val_accuracy'])\n",
    "    plt.title('Modelo de precisión')\n",
    "    plt.ylabel('Precisión')\n",
    "    plt.xlabel('Epoca')\n",
    "    plt.legend(['Entrenamiento', 'Prueba'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(historial.history['loss'])\n",
    "    plt.plot(historial.history['val_loss'])\n",
    "    plt.title('Modelo de pérdida')\n",
    "    plt.ylabel('Pérdida')\n",
    "    plt.xlabel('Epoca')\n",
    "    plt.legend(['Entrenamiento', 'Prueba'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    return modelo\n",
    "\n",
    "palabras, categorias, documentos = tokenizador()\n",
    "palabras = lematizador(palabras, categorias, documentos)\n",
    "x_entrenamiento, y_entrenamiento = entrenamiento(palabras, categorias, documentos)\n",
    "modelo = construir_modelo(x_entrenamiento, y_entrenamiento, categorias)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
