{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import nltk\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "\n",
    "stemmer = SnowballStemmer ('spanish')\n",
    "ignorar_palabras = [\"?\", \"¿\", \"!\", \"¡\"]\n",
    "archivos_datos = open(\"intenciones.json\").read()\n",
    "intenciones = json.load(archivos_datos)\n",
    "\n",
    "def tokenizador():\n",
    "    palabras = []\n",
    "    clases = []\n",
    "    documentos = []\n",
    "\n",
    "    for intencion in intenciones [\"intenciones\"]:\n",
    "      for patron in intencion [\"patrones\"]:\n",
    "        w = nltk.word_tokenize(patron)\n",
    "        palabras.extend(w)\n",
    "        documentos.append((w, intencion[\"categoria\"]))\n",
    "        if intencion[\"categoria\"] not in clases:\n",
    "            clases.append(intencion[\"categoria\"])\n",
    "\n",
    "    return palabras, clases, documentos\n",
    "\n",
    "def lamantizador(palabras, clases, documentos):\n",
    "    palabras = [stemmer.stem(w.lower()) for w in palabras if w not in ignorar_palabras]\n",
    "    palabras2 = palabras\n",
    "    pickle.dump(palabras, open(\"palabras.pkl\", \"wb\"))\n",
    "    pickle.dump(clases, open(\"classes.pkl\", \"wb\"))\n",
    "  \n",
    "    return palabras2\n",
    "\n",
    "def entrenamiento (palabras, clases, documentos):\n",
    "    entrenamiento = []\n",
    "    salida_vacia = [0] * len(clases)\n",
    "    for doc in documentos:\n",
    "      bolsa = []\n",
    "      palabras_patron = doc[0]\n",
    "      palabras_patron = [stemmer.stem(word.lower()) for word in palabras_patron if word not in ignorar_palabras]\n",
    "      for w in palabras:\n",
    "        bolsa.append(1) if w in palabras_patron else bolsa.append(0)\n",
    "        salida_fila = list (salida_fila)\n",
    "        salida_fila [clases.index(doc[1])] = 1\n",
    "        entrenamiento = np.array(entrenamiento, dtype = object)\n",
    "        x_entrenamiento = list(entrenamiento[:,0])\n",
    "        y_entrenamiento = list(entrenamiento[:,1])\n",
    "        return x_entrenamiento, y_entrenamiento\n",
    "\n",
    "def contruir_modelo(x_entrenamiento, y_entrenamiento, clases):\n",
    "    modelo = Sequential()\n",
    "    modelo.add(Dense(128, input_shape = (len(x_entrenamiento[0]),), activation='relu'))\n",
    "    modelo.add(Dropout(0.5))\n",
    "    modelo.add(Dense(64, activation='relu'))\n",
    "    modelo.add(Dropout(0.5))\n",
    "    modelo.add(Dense(len(clases), activation = 'softmax'))\n",
    "    modelo.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "    historia = modelo.fit(np.array(x_entrenamiento), np.array(y_entrenamiento), epochs=50, batch_size = 5, verbose=1, validation_split=0.16)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    #Grafica de los valores de precisión de entrenamiento y validación\n",
    "    plt.plot(historia.history['accuracy'])\n",
    "    plt.plot(historia.history['val_accuracy'])\n",
    "    plt.title('Modelo accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Entrenamiento', 'Prueba'], loc = 'upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    #Grafica de valores de las pérdidas de entrenamiento y validación\n",
    "    plt.plot(historia.history['loss'])\n",
    "    plt.plot(historia.history['val_loss'])\n",
    "    plt.title('Modelo loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Entrenamiento', 'Prueba'], loc='upper left')\n",
    "\n",
    "    #construye el modelo y lo guarda\n",
    "    modelo.save('modelo.h5')\n",
    "    return modelo\n",
    "\n",
    "    x_entrenamiento, y_entrenamiento = entrenamiento(lamentizador(tokenizador()))\n",
    "    constrir_modelo(x_entrenamiento, y_entrenamiento, lamentizador(tokenizador())[1])\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
